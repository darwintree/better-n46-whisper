{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get hugging face token here https://huggingface.co/settings/tokens\n",
    "- Click agree in pyannote projects including [Segmentation](https://huggingface.co/pyannote/segmentation) , [Voice Activity Detection (VAD)](https://huggingface.co/pyannote/voice-activity-detection), and [Speaker Diarization](https://huggingface.co/pyannote/speaker-diarization)\n",
    "\n",
    "\n",
    "- 在这里获取你的 Hugging Face token https://huggingface.co/settings/tokens\n",
    "- 在这些项目中同意协议：[Segmentation](https://huggingface.co/pyannote/segmentation) , [Voice Activity Detection (VAD)](https://huggingface.co/pyannote/voice-activity-detection), [Speaker Diarization](https://huggingface.co/pyannote/speaker-diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = \"./raw\" # the foler of the source video/audio file\n",
    "OUTPUT_DIR = \"./out\" # the folder where the output will be placed\n",
    "MIN_SPEAKERS = 1\n",
    "MAX_SPEAKERS = 7\n",
    "CHUNK_SIZE = 10 # lower chunk_size will make each sentence shorter\n",
    "diarize = True\n",
    "\n",
    "# disabled\n",
    "# vad_onset = 0.05\n",
    "# vad_offset = 0\n",
    "\n",
    "diarize_param = \"--diarize\" if diarize else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", None) or \"fill your hf token here/将此字符串替换为你的hf token\"\n",
    "raw_file = \"u149.mp4\"\n",
    "openai_key = os.environ.get(\"OPENAI_KEY\", None) or \"fill your openai key here/将此字符串替换为你的openai key\"\n",
    "api_base = os.environ.get(\"API_BASE\", None) or \"https://api.openai.com/v1\" # replace with your proxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.8. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/whisperx-vad-segmentation.bin`\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n",
      ">>Performing transcription...\n",
      ">>Performing alignment...\n",
      ">>Performing diarization...\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.8. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -m whisperx --model large-v2 --language ja {RAW_DIR}/{raw_file} --hf_token {hf_token} --output_dir {OUTPUT_DIR} --align_model jonatasgrosman/wav2vec2-large-xlsr-53-japanese --max_line_width 30 --min_speakers {MIN_SPEAKERS} --max_speakers {MAX_SPEAKERS}  {diarize_param} --chunk_size {CHUNK_SIZE} --output_format json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubs2, json\n",
    "from typing import cast\n",
    "from better_n46_whisper.TYPES import Segment\n",
    "\n",
    "ext = os.path.splitext(raw_file)[1]\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/{os.path.basename(raw_file).replace(ext, '.json')}\") as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "subs = pysubs2.load_from_whisper(result)\n",
    "subs.save(f\"{OUTPUT_DIR}/{os.path.basename(raw_file).replace(ext, '.ass')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_by_speaker: dict[str, set] = {}\n",
    "for segment in result[\"segments\"]:\n",
    "    segment = cast(Segment, segment)\n",
    "    if segment[\"speaker\"] is None:\n",
    "        continue\n",
    "    texts_by_speaker.setdefault(segment[\"speaker\"], set())\n",
    "    texts_by_speaker[segment[\"speaker\"]].add(segment[\"text\"])\n",
    "    \n",
    "style = subs.styles[\"Default\"]\n",
    "style.fontname = \"思源黑体 CN Medium\"\n",
    "style.fontsize = 72\n",
    "# style.primarycolor = \"&H00FFFFFF\"\n",
    "# style.secondarycolor = \"&H000000FF\"\n",
    "# style.outlinecolor = \"&H00000000\"\n",
    "# style.backcolor = \"&H00000000\"\n",
    "style.bold = True\n",
    "# style.scalex = 100\n",
    "# style.scaley = 100\n",
    "# style.spacing = 0\n",
    "# style.angle = 0\n",
    "# style.borderstyle = 1\n",
    "# style.outline = 2\n",
    "# style.shadow = 2\n",
    "# # style.alignment = 2\n",
    "# style.marginl = 10\n",
    "# style.marginr = 10\n",
    "# style.marginv = 10\n",
    "# style.encoding = 1\n",
    "\n",
    "for speaker in texts_by_speaker.keys():\n",
    "    subs.styles[speaker] = subs.styles[\"Default\"].copy()\n",
    "\n",
    "# subs.styles[\"SPEAKER_02\"].primarycolor = Color(194, 225, 137)\n",
    "# subs.styles[\"SPEAKER_03\"].primarycolor = Color(92, 136, 218)\n",
    "\n",
    "for i in range(len(subs)):\n",
    "    speaker = result[\"segments\"][i][\"speaker\"]\n",
    "    subs[i].style = speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_n46_whisper.translator.base import LLMTranslator\n",
    "\n",
    "translator = LLMTranslator(openai_key, temperature=0.5, model=\"gpt-3.5-turbo-16k\", api_base=api_base)\n",
    "translator.translate_ass(subs, batch_size=30)\n",
    "subs.save(f\"{OUTPUT_DIR}/{os.path.basename(raw_file).replace(ext, '-translated.ass')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
